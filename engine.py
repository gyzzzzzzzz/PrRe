
"""
Train and eval functions used in main.py
"""
import math
import sys
import os
import datetime
import json
from typing import Iterable
from pathlib import Path

import torch

import numpy as np

from timm.utils import accuracy
from timm.optim import create_optimizer
from argparse import Namespace
import utils


def train_one_epoch(model: torch.nn.Module, original_model: torch.nn.Module, 
                    criterion, data_loader: Iterable, optimizer: torch.optim.Optimizer, 
                    device: torch.device, epoch: int, max_norm: float = 0,
                    set_training_mode=True, task_id=-1, class_mask=None, args = None,):

    model.train(set_training_mode)  # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    original_model.eval()  # å°†åŸå§‹æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
     
    if args.distributed and utils.get_world_size() > 1: 
        data_loader.sampler.set_epoch(epoch)    # å¯¹äºåˆ†å¸ƒå¼è®­ç»ƒï¼Œè®¾ç½®æ•°æ®åŠ è½½å™¨çš„epoch

    metric_logger = utils.MetricLogger(delimiter="  ")     # åˆ›å»ºåº¦é‡è®°å½•å™¨ç”¨äºè·Ÿè¸ªè®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    metric_logger.add_meter('Lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))  # æ·»åŠ å­¦ä¹ ç‡çš„åº¦é‡
    metric_logger.add_meter('Loss', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))   # æ·»åŠ æŸå¤±çš„åº¦é‡
    header = f'Task:{task_id+1} Train: Epoch[{epoch+1:{int(math.log10(args.epochs))+1}}/{args.epochs}]'   # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºçš„æ ‡é¢˜

   
    # éå†æ•°æ®åŠ è½½å™¨ï¼Œå¹¶åœ¨æŒ‡å®šé¢‘ç‡ä¸‹è®°å½•åº¦é‡
    for input, target in metric_logger.log_every(data_loader, args.print_freq, header):
        input = input.to(device, non_blocking=True)  # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
        target = target.to(device, non_blocking=True)   # å°†ç›®æ ‡æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š

        with torch.no_grad():
            if original_model is not None:
                output,_,_,_ = original_model(input)
                cls_features = output['pre_logits'] 
            else:
                cls_features = None
        # output:key['similarity','prompt_idx','selected_key','prompt_key_norm','x_embed_norm','reduce_sim','batched_prompt', 'x', 'pre_logits', 'logits'])
        # æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­
        output,d_prompt,f_prompt,pos_embed= model(input, task_id=task_id, cls_features=cls_features, train=set_training_mode)
        k = output['selected_key'] 
        logits = output['logits'] 
        total_reg_loss = 0
        for i in range(3):
            f_prompt_slice=f_prompt[i]
            product = torch.matmul(d_prompt,f_prompt_slice.permute(0, 1, 2, 4,3))
            reg_loss = torch.norm(product, p='fro')
            total_reg_loss += reg_loss
        alpha=0.001
        # åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ ¹æ®ç»™å®šçš„ç±»åˆ«æ©ç ï¼Œå°†ç‰¹å®šä»»åŠ¡ä¸å…³è”çš„ç±»åˆ«çš„é€»è¾‘è¾“å‡ºï¼ˆlogitsï¼‰ä¿®æ”¹ä¸ºéå¸¸å°çš„å€¼ï¼ˆè´Ÿæ— ç©·ï¼‰ï¼Œä»è€Œåœ¨åç»­çš„softmaxæ“ä½œå’ŒæŸå¤±è®¡ç®—ä¸­ï¼Œå®é™…ä¸Šå¿½ç•¥è¿™äº›ç±»åˆ«
        if args.train_mask and class_mask is not None:
            mask = class_mask[task_id]
            not_mask = np.setdiff1d(np.arange(args.nb_classes), mask) 
            not_mask = torch.tensor(not_mask, dtype=torch.int64).to(device)
            logits = logits.index_fill(dim=1, index=not_mask, value=float('-inf')) 
        # è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼‰
        loss_ce = criterion(logits, target) # base criterion (CrossEntropyLoss)   # åº”ç”¨æ‹‰åŠ¨çº¦æŸï¼ˆå¦‚æœæŒ‡å®šï¼‰
        def euclidean_distance(a, b):
            return ((a - b)**2).sum().sqrt()
        loss_dist=euclidean_distance(k,cls_features)
        ğœ† = 0.005 # ä½ éœ€è¦æ ¹æ®é—®é¢˜æ¥è®¾ç½®åˆé€‚çš„æƒé‡
        total_loss = loss_ce+ğœ†*loss_dist+alpha*total_reg_loss
        if args.pull_constraint and 'reduce_sim' in output:
            total_loss = total_loss - args.pull_constraint_coeff * output['reduce_sim']

        acc1, acc5 = accuracy(logits, target, topk=(1, 5))   # è®¡ç®—top-1å’Œtop-5å‡†ç¡®ç‡

        if not math.isfinite(total_loss.item()):
            print("Loss is {}, stopping training".format(total_loss.item()))
            sys.exit(1)

        optimizer.zero_grad()   # æ¢¯åº¦æ¸…é›¶
        total_loss.backward()     # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)   # å¯¹æ¢¯åº¦è¿›è¡Œè£å‰ª
        optimizer.step()  # æ›´æ–°æ¨¡å‹å‚
        torch.cuda.synchronize()   # åŒæ­¥CUDAæµ
        metric_logger.update(Loss=total_loss.item())   # æ›´æ–°æŸå¤±
        metric_logger.update(Lr=optimizer.param_groups[0]["lr"])  #æ›´æ–°å­¦ä¹ ç‡
        metric_logger.meters['Acc@1'].update(acc1.item(), n=input.shape[0])   # æ›´æ–°top-1å‡†ç¡®ç‡
        metric_logger.meters['Acc@5'].update(acc5.item(), n=input.shape[0])   # æ›´æ–°top-5å‡†ç¡®ç‡
        
     # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„ç»Ÿè®¡ä¿¡æ¯
    metric_logger.synchronize_between_processes()
    print("Averaged stats:", metric_logger)  # æ‰“å°å¹³å‡ç»Ÿè®¡ä¿¡æ¯
    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}  # è¿”å›å…¨å±€å¹³å‡ç»Ÿè®¡ä¿¡æ¯

@torch.no_grad()
def evaluate(model: torch.nn.Module, original_model: torch.nn.Module, data_loader, 
            device, task_id=-1, class_mask=None, args=None,):  # æ‰“å°å¹³å‡ç»Ÿè®¡ä¿¡æ¯
    criterion = torch.nn.CrossEntropyLoss()   # è¿”å›å…¨å±€å¹³å‡ç»Ÿè®¡ä¿¡æ¯

    metric_logger = utils.MetricLogger(delimiter="  ")
    header = 'Test: [Task {}]'.format(task_id + 1)

    # switch to evaluation mode
    model.eval()
    original_model.eval()

    with torch.no_grad():
        for input, target in metric_logger.log_every(data_loader, args.print_freq, header):
            input = input.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)

            # compute output

            if original_model is not None:
                output = original_model(input)
                cls_features = output['pre_logits']
            else:
                cls_features = None
            
            output = model(input, task_id=task_id, cls_features=cls_features)
            logits = output['logits']

            if args.task_inc and class_mask is not None:
                #adding mask to output logits
                mask = class_mask[task_id]
                mask = torch.tensor(mask, dtype=torch.int64).to(device)
                logits_mask = torch.ones_like(logits, device=device) * float('-inf')
                logits_mask = logits_mask.index_fill(1, mask, 0.0)
                logits = logits + logits_mask

            loss = criterion(logits, target)

            acc1, acc5 = accuracy(logits, target, topk=(1, 5))

            metric_logger.meters['Loss'].update(loss.item())
            metric_logger.meters['Acc@1'].update(acc1.item(), n=input.shape[0])
            metric_logger.meters['Acc@5'].update(acc5.item(), n=input.shape[0])

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    print('* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'
          .format(top1=metric_logger.meters['Acc@1'], top5=metric_logger.meters['Acc@5'], losses=metric_logger.meters['Loss']))

    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}


@torch.no_grad() 
def evaluate_till_now(model: torch.nn.Module, original_model: torch.nn.Module, data_loader, 
                    device, task_id=-1, class_mask=None, acc_matrix=None, args=None,):
    stat_matrix = np.zeros((3, args.num_tasks)) 

    for i in range(task_id+1):
        test_stats = evaluate(model=model, original_model=original_model, data_loader=data_loader[i]['val'], 
                            device=device, task_id=i, class_mask=class_mask, args=args)

        stat_matrix[0, i] = test_stats['Acc@1']
        stat_matrix[1, i] = test_stats['Acc@5']
        stat_matrix[2, i] = test_stats['Loss']

        acc_matrix[i, task_id] = test_stats['Acc@1']
    
    avg_stat = np.divide(np.sum(stat_matrix, axis=1), task_id+1)

    diagonal = np.diag(acc_matrix)

    result_str = "[Average accuracy till task{}]\tAcc@1: {:.4f}\tAcc@5: {:.4f}\tLoss: {:.4f}".format(task_id+1, avg_stat[0], avg_stat[1], avg_stat[2])
    if task_id > 0:
        forgetting = np.mean((np.max(acc_matrix, axis=1) -
                            acc_matrix[:, task_id])[:task_id])
        backward = np.mean((acc_matrix[:, task_id] - diagonal)[:task_id])

        result_str += "\tForgetting: {:.4f}\tBackward: {:.4f}".format(forgetting, backward)
    
    print(result_str)

    return test_stats
# å®šä¹‰äº†ä¸€ä¸ª train_and_evaluate çš„å‡½æ•°ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
def train_and_evaluate(model: torch.nn.Module, model_without_ddp: torch.nn.Module, original_model: torch.nn.Module, 
                    criterion, data_loader: Iterable, optimizer: torch.optim.Optimizer, lr_scheduler, device: torch.device, 
                    class_mask=None, args = None,):

    # åˆ›å»ºçŸ©é˜µä»¥ä¿å­˜ä»»åŠ¡ç»“æŸæ—¶çš„å‡†ç¡®åº¦ 
    acc_matrix = np.zeros((args.num_tasks, args.num_tasks))  
    all_logits =None
    all_targets =None
    prototype = utils.ClassPrototypes(num_class=args.nb_classes,dim=args.nb_classes,device=device) 
    
    #åˆ›å»ºä¸€ä¸ªç”¨äºå­˜å‚¨ç±»åˆ«åŸå‹çš„å¯¹è±¡ï¼Œå¹¶ä¸”æŒ‡å®šäº†ç±»åˆ«çš„æ•°é‡ã€ç»´åº¦å’Œè®¡ç®—è®¾å¤‡
         # å¾ªç¯æ¯ä¸ªä»»åŠ¡
    for task_id in range(args.num_tasks): 
        
        # Transfer previous learned prompt params to the new prompt
         # å¦‚æœå½“å‰ä»»åŠ¡éœ€è¦å…±äº«å…ˆå‰å­¦ä¹ çš„æç¤ºå‚æ•°åˆ°æ–°çš„æç¤º
        if args.prompt_pool and args.shared_prompt_pool and args.use_f_prompt:   # è¿›è¡Œä¸€äº›æ¡ä»¶æ£€æŸ¥åï¼Œå°†å…ˆå‰å­¦ä¹ çš„æç¤ºå‚æ•°ä¼ é€’åˆ°æ–°çš„æç¤ºä¸­
            if task_id > 0: 
                prev_start = (task_id - 1) * args.top_k 
                prev_end = task_id * args.top_k 

                cur_start = prev_end 
                cur_end = (task_id + 1) * args.top_k 

                if (prev_end > args.size) or (cur_end > args.size):
                    pass
                else:
                    cur_idx = (slice(None), slice(None), slice(cur_start, cur_end)) if args.use_prefix_tune_for_f_prompt else (slice(None), slice(cur_start, cur_end))
                    prev_idx = (slice(None), slice(None), slice(prev_start, prev_end)) if args.use_prefix_tune_for_f_prompt else (slice(None), slice(prev_start, prev_end))

                    with torch.no_grad(): 
                        if args.distributed:
                            model.module.f_prompt.prompt.grad.zero_()
                            model.module.f_prompt.prompt[cur_idx] = model.module.f_prompt.prompt[prev_idx]
                            optimizer.param_groups[0]['params'] = model.module.parameters()
                        else:
                            model.f_prompt.prompt.grad.zero_()
                            model.f_prompt.prompt[cur_idx] = model.f_prompt.prompt[prev_idx]
                            optimizer.param_groups[0]['params'] = model.parameters()
                    
        #å¦‚æœå½“å‰ä»»åŠ¡éœ€è¦å…±äº«å…ˆå‰å­¦ä¹ çš„æç¤ºå‚æ•°é”®åˆ°æ–°çš„æç¤º
        if args.prompt_pool and args.shared_prompt_key and args.use_f_prompt: # è¿›è¡Œä¸€äº›æ¡ä»¶æ£€æŸ¥åï¼Œå°†å…ˆå‰å­¦ä¹ çš„æç¤ºå‚æ•°é”®ä¼ é€’åˆ°æ–°çš„æç¤ºä¸­
            if task_id > 0: 
                with torch.no_grad():
                    if args.distributed:
                        model.module.f_prompt.prompt_key.grad.zero_()
                        model.module.f_prompt.prompt_key[cur_idx] = model.module.f_prompt.prompt_key[prev_idx]
                        optimizer.param_groups[0]['params'] = model.module.parameters()
                    else:
                        model.f_prompt.prompt_key.grad.zero_()
                        model.f_prompt.prompt_key[cur_idx] = model.f_prompt.prompt_key[prev_idx]
                        optimizer.param_groups[0]['params'] = model.parameters()

        # å¦‚æœéœ€è¦ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºæ–°çš„ä¼˜åŒ–å™¨ä»¥æ¸…é™¤ä¼˜åŒ–å™¨çŠ¶æ€
        if task_id > 0 and args.reinit_optimizer: # å¦‚æœæ¡ä»¶æ»¡è¶³ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„ä¼˜åŒ–å™¨
            optimizer = create_optimizer(args, model) 
        # å¦‚æœéœ€è¦æ›´æ–°ä»»åŠ¡çš„è®­ç»ƒè½®æ¬¡
        if task_id > 0:
            try: 
                args.epochs = args.inc_epochs
            except:
                pass

        # å¾ªç¯æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒè½®æ¬¡
        for epoch in range(args.epochs):       # å¯¹æ¨¡å‹è¿›è¡Œä¸€è½®è®­ç»ƒ     
            train_stats = train_one_epoch(model=model, original_model=original_model, criterion=criterion, 
                                        data_loader=data_loader[task_id]['train'], optimizer=optimizer, 
                                        device=device, epoch=epoch, max_norm=args.clip_grad, 
                                        set_training_mode=True, task_id=task_id, class_mask=class_mask, args=args,)
            # å¦‚æœå­˜åœ¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œåˆ™è°ƒæ•´å­¦ä¹ ç‡
            if lr_scheduler:
                lr_scheduler.step(epoch)
       
        logits,targets=utils.logits_and_targets(model=model,original_model=original_model,
                                data_loader=data_loader[task_id]['train'],task_id=task_id,device=device,)
        if task_id==0:
            all_logits=logits
            all_targets=targets
        else:
        # ä¹‹åçš„è¿­ä»£ï¼Œå°†æ–°ç»“æœæ’å…¥åˆ°all_logitså’Œall_targetsçš„å¤´éƒ¨
            all_logits = torch.cat((all_logits, logits), dim=0)
            all_targets = torch.cat((all_targets, targets), dim=0)
        # Prototype Variable
        
        # æ›´æ–°ä»»åŠ¡çš„åŸå‹å¹¶è®¡ç®—å‡†ç¡®åº¦çŸ©é˜µ
        prototype = utils.prototype_updata(prototype=prototype, model=model,original_model=original_model,device=device, data_loader=data_loader[task_id]['train'],task_id=task_id,)
        acc_matrix = utils.prototype_evaluate_till_now(prototype=prototype,all_logits=all_logits,all_targets=all_targets, model=model, original_model=original_model, data_loader=data_loader, device=device, 
                                    task_id=task_id, class_mask=class_mask, acc_matrix=acc_matrix, args=args,)
        
        # MLP Classifier
        # test_stats = evaluate_till_now(model=model, original_model=original_model, data_loader=data_loader, device=device, 
        #                             task_id=task_id, class_mask=class_mask, acc_matrix=acc_matrix, args=args)

        # if args.output_dir and utils.is_main_process():
        #     Path(os.path.join(args.output_dir, 'checkpoint')).mkdir(parents=True, exist_ok=True)

        #     checkpoint_path = os.path.join(args.output_dir, 'checkpoint/task{}_checkpoint.pth'.format(task_id+1))
            
        #     args_to_save = Namespace(**{k: v for k, v in vars(args).items() if k != "Dataset"}) 
        #     state_dict = {
        #             'model': model_without_ddp.state_dict(),
        #             'optimizer': optimizer.state_dict(),
        #             'epoch': epoch,
        #             'args': args_to_save,
        #         }
        #     if args.sched is not None and args.sched != 'constant': 
        #         state_dict['lr_scheduler'] = lr_scheduler.state_dict()
            
        #     utils.save_on_master(state_dict, checkpoint_path)

        # log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},
        #     **{f'test_{k}': v for k, v in test_stats.items()},
        #     'epoch': epoch,}

        # if args.output_dir and utils.is_main_process():
        #     with open(os.path.join(args.output_dir, '{}_stats.txt'.format(datetime.datetime.now().strftime('log_%Y_%m_%d_%H_%M'))), 'a') as f:
        #         f.write(json.dumps(log_stats) + '\n')

     # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„å¹³å‡å‡†ç¡®åº¦
    column_mean_values = []
    for i in range(args.num_tasks):
        selected_elements = acc_matrix[:i+1, i]
        mean_value = np.mean(selected_elements)
        column_mean_values.append(mean_value)
# æ‰“å°å‡†ç¡®åº¦çŸ©é˜µå’Œæ¯ä¸ªä»»åŠ¡çš„å¹³å‡å‡†ç¡®åº¦å€¼
    print(acc_matrix)
    print(column_mean_values)
    
    return acc_matrix
